---
title: 01.spider使用
description: spider使用
published: 1
date: 2023-05-29T04:12:00.821Z
tags: golang
editor: markdown
dateCreated: 2023-04-22T04:53:18.743Z
---

<center>爬虫</center>



[toc]



## 爬虫

> 爬取数据源，重新建立索引。`google & baidu`
>
> 数据分析和大数据



### 1. http包实现

```go
func firstSpider(url string) string {
	// 客户端
	client := &http.Client{}
	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		log.Printf(err.Error())
	}

	// 设置请求头  博客园的cookie
	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36")
	// 登录所需的cookie
    req.Header.Add("cookie", "")

	// 发送请求
	resp, err := client.Do(req)
	if err != nil {
		log.Printf("err:", err)
		return ""
	}

	if resp.StatusCode != 200 {
		log.Printf("code:", resp.StatusCode)
		return ""
	}
	defer resp.Body.Close()

	bodystr, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Printf("err", err)
		return ""
	}

	return string(bodystr)
}

func main() {
	url := "https://zzk.cnblogs.com/s/blogpost?w=kaka"
	s := firstSpider(url)
	fmt.Printf("s: %v\n", s)
}
```



> 牛逼呀，注意别随便爬哦 

```go
package main

import (
	"fmt"
	"io/ioutil"
	"log"
	"net/http"
	"os"
	"regexp"
	"strings"
)

// 发送解析   获取到请求内容
func firstSpider(url string) string {
	// 客户端
	client := &http.Client{}
	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		log.Printf(err.Error())
	}

	// 设置请求头  博客园的cookie
	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36")

	// 发送请求
	resp, err := client.Do(req)
	if err != nil {
		log.Printf("err:", err)
		return ""
	}

	if resp.StatusCode != 200 {
		log.Printf("code:", resp.StatusCode)
		return ""
	}
	defer resp.Body.Close()

	bodystr, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Printf("err", err)
		return ""
	}
	return string(bodystr)
}

// 获取到文章标题
func getAtr(html string) {
	// 替换 换行 空格
	html = strings.Replace(html, "\n", "", -1)

	// 获取到标签内的内容
	re_sidebar := regexp.MustCompile(`<aside id="sidebar" role="navigation">(.*?)</aside>`)
	// 找到侧边栏的内容块
	siderbar := re_sidebar.FindString(html)
	// a链接的正则
	re_like := regexp.MustCompile(`href="(.*?)"`)
	// 所有链接
	likes := re_like.FindAllString(siderbar, -1)
	// url
	base_url := "https://gorm.io/zh_CN/docs/"
	for _, v := range likes {
		// 切片获取到  href="index.html"
		s := v[6 : len(v)-1]
		// 拼接 连接  https://gorm.io/zh_CN/docs/gorm_config.html
		url := base_url + s
		fmt.Printf("url: %v\n", url)

		// 获取内容
		body := firstSpider(url)
		// 开启新线程
		go getContent(body)
	}
}

// 获取内容
func getContent(body string) {
	// 替换换行
	body = strings.Replace(body, "\n", "", -1)
	// 找到页面
	re_content := regexp.MustCompile(`<div class="article">(.*?)</div>`)
	// 找到页面内容
	content := re_content.FindString(body)

	// 标题
	re_title := regexp.MustCompile(`<h1 class="article-title" itemprop="name">(.*?)</h1>`)
	// 内容里面找标题
	title := re_title.FindString(content)
	// 切片保存
	title = title[42 : len(title)-5]
	fmt.Printf("title: %v\n", title)
	saveFile(title, content)
}

// 保存到本地
func saveFile(title string, content string) {
	// 新建文件夹保存
	err := os.WriteFile("./pages/"+title+".html", []byte(content), 0644)
	if err != nil {
		panic(err)
	}
}
func main() {
	url := "https://gorm.io/zh_CN/docs/"
	s := firstSpider(url)
	getAtr(s)
}

```





